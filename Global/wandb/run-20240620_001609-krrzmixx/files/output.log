Resuming from epoch 0 at iteration 0
CustomDatasetDataLoader
dataset [PairOldPhotos] was created
start load bigfile (1.75 GB) into memory
find total 2000 images
load 0 images done
load all 2000 images done
-------------Filter the imgs whose size <256 in VOC-------------
Image read error for index 9: 1007.jpg
Image read error for index 203: 1220.jpg
Image read error for index 232: 1252.jpg
Image read error for index 308: 133.jpg
Image read error for index 393: 1423.jpg
Image read error for index 407: 1438.jpg
Image read error for index 411: 1442.jpg
Image read error for index 447: 1483.jpg
Image read error for index 526: 1566.jpg
/home/quan/miniconda3/envs/photo/lib/python3.8/site-packages/PIL/Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
Image read error for index 553: 1595.jpg
Image read error for index 718: 177.jpg
Image read error for index 795: 1847.jpg
Image read error for index 817: 1871.jpg
Image read error for index 905: 1967.jpg
Image read error for index 974: 2043.jpg
Image read error for index 1048: 2123.jpg
Image read error for index 1067: 2144.jpg
Image read error for index 1072: 2150.jpg
Image read error for index 1078: 2159.jpg
Image read error for index 1184: 2266.jpg
Image read error for index 1198: 2281.jpg
Image read error for index 1240: 2328.jpg
Image read error for index 1347: 2447.jpg
Image read error for index 1359: 2459.jpg
Image read error for index 1480: 2577.jpg
Image read error for index 1490: 2587.jpg
Image read error for index 1504: 26.jpg
Image read error for index 1561: 2658.jpg
Image read error for index 1666: 2766.jpg
Image read error for index 1802: 2905.jpg
Image read error for index 1831: 2933.jpg
Image read error for index 1884: 2990.jpg
Image read error for index 1932: 3040.jpg
Image read error for index 1987: 3099.jpg
--------Origin image num is [2000], filtered result is [1987]--------
#training images = 1984
Mapping: You are using the mapping model without global restoration.
/home/quan/workspace/image-restoration/Global/global_checkpoints/checkpoints/restoration/VAE_A_quality/10_net_G.pth not exists yet!
/home/quan/workspace/image-restoration/Global/global_checkpoints/checkpoints/restoration/VAE_B_quality/10_net_G.pth not exists yet!
MultiscaleDiscriminator(
  (scale0_layer0): Sequential(
    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (scale0_layer1): Sequential(
    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))
    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (scale0_layer2): Sequential(
    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))
    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (scale0_layer3): Sequential(
    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))
    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (scale0_layer4): Sequential(
    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))
  )
  (scale1_layer0): Sequential(
    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (scale1_layer1): Sequential(
    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))
    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (scale1_layer2): Sequential(
    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))
    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (scale1_layer3): Sequential(
    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))
    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (scale1_layer4): Sequential(
    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))
  )
  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])
)
/home/quan/miniconda3/envs/photo/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/quan/miniconda3/envs/photo/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
L1Loss()
---------- Optimizers initialized -------------
/home/quan/workspace/image-restoration/Global/models/networks.py:808: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
