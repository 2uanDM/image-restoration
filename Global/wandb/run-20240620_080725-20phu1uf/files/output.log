Resuming from epoch 0 at iteration 0
CustomDatasetDataLoader
dataset [PairOldPhotos] was created
start load bigfile (1.29 GB) into memory
find total 1500 images
load 0 images done
load all 1500 images done
-------------Filter the imgs whose size <256 in VOC-------------
Image read error for index 9: 1007.jpg
Image read error for index 203: 1220.jpg
Image read error for index 232: 1252.jpg
Image read error for index 308: 133.jpg
Image read error for index 393: 1423.jpg
Image read error for index 407: 1438.jpg
Image read error for index 411: 1442.jpg
Image read error for index 447: 1483.jpg
Image read error for index 526: 1566.jpg
/home/quan/miniconda3/envs/photo/lib/python3.8/site-packages/PIL/Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  warnings.warn(
Image read error for index 553: 1595.jpg
Image read error for index 718: 177.jpg
Image read error for index 795: 1847.jpg
Image read error for index 817: 1871.jpg
Image read error for index 905: 1967.jpg
Image read error for index 974: 2043.jpg
Image read error for index 1048: 2123.jpg
Image read error for index 1067: 2144.jpg
Image read error for index 1072: 2150.jpg
Image read error for index 1078: 2159.jpg
Image read error for index 1184: 2266.jpg
Image read error for index 1198: 2281.jpg
Image read error for index 1240: 2328.jpg
Image read error for index 1347: 2447.jpg
Image read error for index 1359: 2459.jpg
Image read error for index 1480: 2577.jpg
Image read error for index 1490: 2587.jpg
--------Origin image num is [1500], filtered result is [1491]--------
#training images = 1488
Mapping: You are using the mapping model without global restoration.
/home/quan/workspace/image-restoration/Global/global_checkpoints/checkpoints/restoration/VAE_A_quality/10_net_G.pth not exists yet!
/home/quan/workspace/image-restoration/Global/global_checkpoints/checkpoints/restoration/VAE_B_quality/10_net_G.pth not exists yet!
/home/quan/miniconda3/envs/photo/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/quan/miniconda3/envs/photo/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
MultiscaleDiscriminator(
  (scale0_layer0): Sequential(
    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (scale0_layer1): Sequential(
    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))
    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (scale0_layer2): Sequential(
    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))
    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (scale0_layer3): Sequential(
    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))
    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (scale0_layer4): Sequential(
    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))
  )
  (scale1_layer0): Sequential(
    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (scale1_layer1): Sequential(
    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))
    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (scale1_layer2): Sequential(
    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))
    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (scale1_layer3): Sequential(
    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))
    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (scale1_layer4): Sequential(
    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))
  )
  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])
)
L1Loss()
---------- Optimizers initialized -------------
/home/quan/workspace/image-restoration/Global/models/networks.py:808: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)
(epoch: 1, iters: 16, time: 0.970 lr: 0.00020) G_Feat_L2: 97.751 G_GAN: 5.712 G_GAN_Feat: 14.792 G_VGG: 10.374 D_real: 6.068 D_fake: 2.559
(epoch: 1, iters: 400, time: 0.595 lr: 0.00020) G_Feat_L2: 88.548 G_GAN: 3.093 G_GAN_Feat: 16.664 G_VGG: 10.291 D_real: 2.797 D_fake: 2.076
(epoch: 1, iters: 800, time: 0.751 lr: 0.00020) G_Feat_L2: 87.696 G_GAN: 1.691 G_GAN_Feat: 12.388 G_VGG: 7.893 D_real: 1.479 D_fake: 1.303
(epoch: 1, iters: 1200, time: 0.711 lr: 0.00020) G_Feat_L2: 83.204 G_GAN: 1.549 G_GAN_Feat: 15.229 G_VGG: 8.235 D_real: 0.998 D_fake: 0.868
End of epoch 1 / 250 	 Time Taken: 0:24:14.974500
saving the model at the end of epoch 1, iters 1488
(epoch: 2, iters: 16, time: 0.465 lr: 0.00020) G_Feat_L2: 81.105 G_GAN: 2.753 G_GAN_Feat: 15.616 G_VGG: 9.095 D_real: 1.440 D_fake: 2.406
(epoch: 2, iters: 112, time: 0.695 lr: 0.00020) G_Feat_L2: 78.928 G_GAN: 2.408 G_GAN_Feat: 13.341 G_VGG: 9.363 D_real: 2.018 D_fake: 1.697
(epoch: 2, iters: 512, time: 0.684 lr: 0.00020) G_Feat_L2: 82.426 G_GAN: 1.361 G_GAN_Feat: 14.335 G_VGG: 8.931 D_real: 1.023 D_fake: 1.598
(epoch: 2, iters: 912, time: 0.662 lr: 0.00020) G_Feat_L2: 83.210 G_GAN: 1.062 G_GAN_Feat: 15.300 G_VGG: 8.440 D_real: 0.399 D_fake: 0.378
(epoch: 2, iters: 1312, time: 0.696 lr: 0.00020) G_Feat_L2: 86.063 G_GAN: 2.814 G_GAN_Feat: 15.392 G_VGG: 10.452 D_real: 1.947 D_fake: 0.803
End of epoch 2 / 250 	 Time Taken: 0:17:20.723480
saving the model at the end of epoch 2, iters 2976
(epoch: 3, iters: 16, time: 0.610 lr: 0.00020) G_Feat_L2: 86.168 G_GAN: 0.731 G_GAN_Feat: 16.857 G_VGG: 10.059 D_real: 0.866 D_fake: 1.190
(epoch: 3, iters: 224, time: 0.725 lr: 0.00020) G_Feat_L2: 82.071 G_GAN: 2.712 G_GAN_Feat: 16.009 G_VGG: 10.004 D_real: 1.129 D_fake: 0.566
(epoch: 3, iters: 624, time: 0.713 lr: 0.00020) G_Feat_L2: 84.813 G_GAN: 0.813 G_GAN_Feat: 16.717 G_VGG: 10.123 D_real: 0.562 D_fake: 1.056
(epoch: 3, iters: 1024, time: 0.705 lr: 0.00020) G_Feat_L2: 85.210 G_GAN: 1.678 G_GAN_Feat: 17.421 G_VGG: 9.382 D_real: 0.356 D_fake: 0.424
(epoch: 3, iters: 1424, time: 0.693 lr: 0.00020) G_Feat_L2: 83.920 G_GAN: 1.279 G_GAN_Feat: 15.085 G_VGG: 9.573 D_real: 0.342 D_fake: 0.418
End of epoch 3 / 250 	 Time Taken: 0:17:35.413618
saving the model at the end of epoch 3, iters 4464
(epoch: 4, iters: 16, time: 0.522 lr: 0.00020) G_Feat_L2: 81.283 G_GAN: 2.095 G_GAN_Feat: 15.470 G_VGG: 8.823 D_real: 0.804 D_fake: 0.466
(epoch: 4, iters: 336, time: 0.724 lr: 0.00020) G_Feat_L2: 81.333 G_GAN: 0.956 G_GAN_Feat: 17.832 G_VGG: 10.175 D_real: 0.529 D_fake: 0.412
(epoch: 4, iters: 736, time: 0.703 lr: 0.00020) G_Feat_L2: 73.278 G_GAN: 1.795 G_GAN_Feat: 17.215 G_VGG: 8.803 D_real: 0.370 D_fake: 0.429
(epoch: 4, iters: 1136, time: 0.767 lr: 0.00020) G_Feat_L2: 83.135 G_GAN: 1.094 G_GAN_Feat: 17.665 G_VGG: 8.841 D_real: 0.457 D_fake: 0.332
Traceback (most recent call last):
  File "train_mapping.py", line 119, in <module>
    loss_G.backward()
  File "/home/quan/miniconda3/envs/photo/lib/python3.8/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/quan/miniconda3/envs/photo/lib/python3.8/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/quan/miniconda3/envs/photo/lib/python3.8/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt